<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Devops on Heartysoft Solutions Limited</title>
    <link>http://heartysoft.com/tags/devops/</link>
    <description>Recent content in Devops on Heartysoft Solutions Limited</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 29 May 2017 16:05:05 +0000</lastBuildDate>
    
	<atom:link href="http://heartysoft.com/tags/devops/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Cross AWS Account Builds - Part 2</title>
      <link>http://heartysoft.com/ashic/blog/2017/05/cross-aws-account-builds-part-2/</link>
      <pubDate>Mon, 29 May 2017 16:05:05 +0000</pubDate>
      
      <guid>http://heartysoft.com/ashic/blog/2017/05/cross-aws-account-builds-part-2/</guid>
      <description>At the end of part 1, we have an IAM role created in our account, OURTHING which has the required permissions in our account, and allows a user in the BUILD account to assume this role provided they specify a matching ExternalId - which in this case is &amp;ldquo;some_key_build&amp;rdquo;.
Steps for BUILDACCOUNT We&amp;rsquo;ll need an IAM user in BUILD account which we can use to run our builds. We&amp;rsquo;ll need the BUILD account admin to create this for us.</description>
    </item>
    
    <item>
      <title>Cross AWS Account Builds - Part 1</title>
      <link>http://heartysoft.com/ashic/blog/2017/05/cross-aws-account-builds-part-1/</link>
      <pubDate>Tue, 23 May 2017 18:27:05 +0000</pubDate>
      
      <guid>http://heartysoft.com/ashic/blog/2017/05/cross-aws-account-builds-part-1/</guid>
      <description>Getting cross AWS account permissioning working seems like a black art. At my current client, we have our own AWS Account, with stuff like Kubernetes, Elasticsearch, Postgres, Cassandra, etc. running. However, another team manages a Gitlab Enterprise set up which is hosting our codebases. We could set up our own source control, and build server, and we very well might do that. But for now, the quickest bang for the buck is to leverage what the other team are allowing us to use.</description>
    </item>
    
    <item>
      <title>Public ELBs with Private Kubernetes</title>
      <link>http://heartysoft.com/ashic/blog/2017/05/public-elbs-with-private-kubernetes/</link>
      <pubDate>Mon, 22 May 2017 18:17:00 +0000</pubDate>
      
      <guid>http://heartysoft.com/ashic/blog/2017/05/public-elbs-with-private-kubernetes/</guid>
      <description>Kubernetes services allow us to address a collection of pods through a stable endpoint. Depending on the service type, this endpoint may be addressable only from within the cluster, from the same network, or even from the public internet. When running on AWS, setting the service type for a service to LoadBalancer will create an AWS ELB, which can be used as the endpoint. Wih a Kubernetes cluster running on public subnets, this just works.</description>
    </item>
    
    <item>
      <title>Upgrading Kubernetes with Kops to 1.6.2</title>
      <link>http://heartysoft.com/ashic/blog/2017/05/upgrading-k8s-with-kops-to-1.6.2-woes/</link>
      <pubDate>Sat, 20 May 2017 20:42:30 +0000</pubDate>
      
      <guid>http://heartysoft.com/ashic/blog/2017/05/upgrading-k8s-with-kops-to-1.6.2-woes/</guid>
      <description>Kops 1.6.0 was released this week, and I went ahead and ran kop update on one of my 1.5.2 clusters. After the rolling upgrade, it appeared to go well, but the master nodes didn&amp;rsquo;t seem to be ready. I tried various things, but the issues seemed to revolved around three components: kube-dns, weave-net, and dns-controller.
Fixing kube-dns was simple. I just created an empty ConfigMap named kube-dns in the kube-system namespace:</description>
    </item>
    
    <item>
      <title>Friendly Names for AWS ELBS</title>
      <link>http://heartysoft.com/ashic/blog/2017/05/friendly-names-for-aws-elbs/</link>
      <pubDate>Sun, 14 May 2017 17:54:05 +0000</pubDate>
      
      <guid>http://heartysoft.com/ashic/blog/2017/05/friendly-names-for-aws-elbs/</guid>
      <description>Elastic Load Balancers (ELBs) in AWS allow us to access load balanced resources via single endpoints. While useful, a common problem is that they have hideous names. A dns name like internal-aaaaaabbbbcc359111e789ac0ac70354142-1111223232.eu-west-1.elb.amazonaws.com isn&amp;rsquo;t the easiest thing in the world to remember, share, or use. We can use AWS Route53 to expose these load balancers with &amp;ldquo;friendly&amp;rdquo; names.
Say, I have a domain, mysite.com registered via Route53 as a public Hosted Zone.</description>
    </item>
    
    <item>
      <title>Kubernetes and Weird Environment Variables</title>
      <link>http://heartysoft.com/ashic/blog/2017/05/kubernetes-and-weird-env-vars/</link>
      <pubDate>Wed, 10 May 2017 00:27:30 +0000</pubDate>
      
      <guid>http://heartysoft.com/ashic/blog/2017/05/kubernetes-and-weird-env-vars/</guid>
      <description>We&amp;rsquo;ve started using Kubernetes, and came across an obvious thing that wasn&amp;rsquo;t apparent until we realised what it was. We were setting up a Kafka Schema Registry, and a Spark cluster on Kubernetes. Things seemed to work, except we suddenly saw that they didn&amp;rsquo;t. Digging down, we saw that the Schema Registry was failing during initialisation stating that SCHEMA_REGISTRY_PORT is deprecated. We couldn&amp;rsquo;t figure out what was injecting the environment variable called SCHEMA_REGISTRY_PORT, and why it suddenly started getting injected, having worked fine before.</description>
    </item>
    
  </channel>
</rss>